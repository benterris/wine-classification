{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchtext import data\n",
    "\n",
    "SEED = 1452 # for reproducibility\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "TEXT = data.Field(tokenize= 'spacy')\n",
    "LABEL = data.LabelField()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['86909', 'Chile', \"A chalky, dusty mouthfeel nicely balances this Petite Syrah's bright, full blackberry and blueberry fruit. Wheat-flour and black-pepper notes add interest to the bouquet; the wine finishes with herb and an acorny nuttiness. A good first Chilean wine for those more comfortable with the Californian style. It's got tannins to lose, but it's very good.\", '', '88', '17.0', 'Maipo Valley', '', '', '', '', 'Carmen 1999  (Maipo Valley)', '', 'Carmen']\n",
      "Removed 1 rows\n",
      "129975\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "import csv\n",
    "\n",
    "with open('datasets/winemag-data-130k-v2.csv') as f:\n",
    "    reader = csv.reader(f)\n",
    "    rows = []\n",
    "    \n",
    "    removed = 0\n",
    "\n",
    "    for row in reader:\n",
    "        variety = row[-2]\n",
    "        if not variety:\n",
    "            print(row)\n",
    "            removed +=1\n",
    "        else:\n",
    "            rows.append(row)\n",
    "            \n",
    "            \n",
    "print(\"Removed \" + str(removed) + \" rows\")\n",
    "    \n",
    "    \n",
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38992\n",
      "64988\n",
      "64987\n",
      "38992\n",
      "25996\n",
      "['18579,US,\"Flavors of candied lemon, lime and pineapple are brightened by crisp acidity in this unoaked  Sauvignon Blanc. The wine reflects this cool-climate Monterey appellation with its clean, brisk character.\",,85,16.0,California,Arroyo Seco,Central Coast,,,Mercy 2010 Sauvignon Blanc (Arroyo Seco),Sauvignon Blanc,Mercy\\n', '89421,US,\"There\\'s a lot of oak on this Chardonnay, to judge by the buttered toast and butterscotch richness. Underneath all that is a wine ripe in tropical fruits and green apples, brightened by excellent, mouthwatering acidity. The oak stands out now, but give the wine until 2015 or 2016 in the cellar to let the parts integrate.\",Sierra Mar Vineyard,90,40.0,California,Santa Lucia Highlands,Central Coast,,,Loring Wine Company 2012 Sierra Mar Vineyard Chardonnay (Santa Lucia Highlands),Chardonnay,Loring Wine Company\\n', '56113,US,\"Almost mauve in color, this widely distributed wine (named after the time the winemaking crew pops open their bottles at home) shows candied strawberry, watermelon Jolly Rancher and a touch of slate on the nose. Tons of tongue-tingling acidity and a expertly grippy texture cut through the watermelon and raspberry flavors.\",515 Vine Select,89,15.0,California,Central Coast,Central Coast,Matt Kettmann,@mattkettmann,Noble Vines 2016 515 Vine Select Rosé (Central Coast),Rosé,Noble Vines\\n']\n"
     ]
    }
   ],
   "source": [
    "# Split in train and test\n",
    "\n",
    "TEST_SET_SIZE = .3\n",
    "VALIDATION_SET_SIZE = .2\n",
    "\n",
    "indices = list(range(1, len(lines)))\n",
    "np.random.seed(SEED)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "first_split_index = int(TEST_SET_SIZE * len(lines))\n",
    "second_split_index = int((TEST_SET_SIZE+VALIDATION_SET_SIZE) * len(lines))\n",
    "\n",
    "print(first_split_index)\n",
    "print(second_split_index)\n",
    "\n",
    "test_indices = indices[:first_split_index]\n",
    "validation_indices = indices[first_split_index:second_split_index]\n",
    "train_indices = indices[second_split_index:]\n",
    "\n",
    "train_set = [lines[k] for k in train_indices]\n",
    "test_set = [lines[k] for k in test_indices]\n",
    "validation_set = [lines[k] for k in validation_indices]\n",
    "\n",
    "print(len(train_set))\n",
    "print(len(test_set))\n",
    "print(len(validation_set))\n",
    "print(train_set[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write split sets\n",
    "with open('preprocessed_datasets/train.csv', 'w') as train_file:\n",
    "    train_file.write(''.join(train_set))\n",
    "with open('preprocessed_datasets/test.csv', 'w') as test_file:\n",
    "    test_file.write(''.join(test_set))\n",
    "with open('preprocessed_datasets/validation.csv', 'w') as validation_file:\n",
    "    validation_file.write(''.join(validation_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the dataset\n",
    "\n",
    "# Put the thing we want to predict as a label\n",
    "tv_datafields = [(\"id\", None),\n",
    "                 (\"country\", LABEL),\n",
    "                 (\"description\", TEXT),\n",
    "                 (\"designation\", None),\n",
    "                 (\"points\", None),\n",
    "                 (\"price\", None),\n",
    "                 (\"province\", None),\n",
    "                 (\"region_1\", None),\n",
    "                 (\"region_2\", None),\n",
    "                 (\"taster_name\", None),\n",
    "                 (\"taster_twitter_handle\", None),\n",
    "                 (\"title\", None),\n",
    "                 (\"variety\", None),\n",
    "                 (\"winery\", None)]\n",
    "\n",
    "trn, vld, tst = data.TabularDataset.splits(path='preprocessed_datasets',\n",
    "                                     format=\"csv\",\n",
    "                                     train= 'train.csv',\n",
    "                                     validation='validation.csv',\n",
    "                                     test='test.csv',\n",
    "                                     fields=tv_datafields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25002\n",
      "47\n"
     ]
    }
   ],
   "source": [
    "# Prepare the vocab (BEWARE: this also downloads the vectors, ~800MB)\n",
    "MAX_VOCAB_SIZE = 25000\n",
    "TEXT.build_vocab(trn,\n",
    "                 max_size=MAX_VOCAB_SIZE,\n",
    "                 vectors = \"glove.6B.100d\", \n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "LABEL.build_vocab(trn)\n",
    "\n",
    "print(len(TEXT.vocab))\n",
    "# 25002 because of <pad> and <unk>\n",
    "print(len(LABEL.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function _default_unk_index at 0x7f85ef13fae8>, {'': 21, 'Chile': 5, 'China': 44, 'Serbia': 35, 'Austria': 7, 'South Africa': 10, 'Czech Republic': 32, 'Switzerland': 37, 'Canada': 14, 'US': 0, 'Greece': 13, 'Bosnia and Herzegovina': 38, 'Georgia': 22, 'Brazil': 27, 'Argentina': 6, 'Bulgaria': 16, 'Italy': 2, '90': 41, 'Bordeaux-style Red Blend': 43, 'Ukraine': 30, 'Peru': 31, 'Lebanon': 28, 'Mexico': 25, 'Macedonia': 34, 'Cyprus': 36, 'Romania': 17, 'Armenia': 42, 'Croatia': 24, 'Luxembourg': 39, 'India': 33, 'Germany': 9, 'Morocco': 29, 'England': 23, 'Australia': 8, 'Slovenia': 19, 'France': 1, 'Hungary': 15, 'Uruguay': 18, ' fine and extremely polished; hold for 10 years.\"': 40, 'Israel': 12, 'Egypt': 45, 'Turkey': 20, 'Spain': 3, 'Portugal': 4, 'Moldova': 26, 'New Zealand': 11, 'Slovakia': 46})\n"
     ]
    }
   ],
   "source": [
    "# print(LABEL.vocab.freqs.most_common(10))\n",
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the iterators\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (trn, vld, tst), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.description), # Sort the examples so the ones with similar lengths are close to each other\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
    "                 dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.convs = nn.ModuleList([\n",
    "                                    nn.Conv2d(in_channels = 1, \n",
    "                                              out_channels = n_filters, \n",
    "                                              kernel_size = (fs, embedding_dim)) \n",
    "                                    for fs in filter_sizes\n",
    "                                    ])\n",
    "        \n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        #text = [sent len, batch size]\n",
    "\n",
    "        text = text.permute(1, 0)        \n",
    "        #text = [batch size, sent len]\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        \n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        #embedded = [batch size, 1, sent len, emb dim]\n",
    "        \n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "        #conv_n = [batch size, n_filters, sent len - filter_sizes[n]]\n",
    "        \n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        #pooled_n = [batch size, n_filters]\n",
    "        \n",
    "        cat = self.dropout(torch.cat(pooled, dim = 1))\n",
    "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
    "            \n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [2,3,4]\n",
    "OUTPUT_DIM = len(LABEL.vocab)\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2604647\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5130,  1.2094,  0.6369,  ..., -0.4520, -0.4385,  0.0610],\n",
       "        [ 0.2348,  0.8024,  2.4040,  ..., -0.0284,  0.4618, -1.6748],\n",
       "        [-0.1077,  0.1105,  0.5981,  ..., -0.8316,  0.4529,  0.0826],\n",
       "        ...,\n",
       "        [-0.3868,  0.9669, -0.1847,  ...,  0.0047, -0.1571,  0.4996],\n",
       "        [ 0.5564, -2.1407,  1.5627,  ...,  0.2384, -0.6747, -0.3413],\n",
       "        [ 0.6508,  2.4764, -0.3841,  ..., -1.1424, -1.8912,  0.5773]])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
    "    correct = max_preds.squeeze(1).eq(y)\n",
    "    return correct.sum() / torch.FloatTensor([y.shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "#     print('start training')\n",
    "    \n",
    "    for batch in iterator:\n",
    "#         print(epoch_loss)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(batch.description)\n",
    "        \n",
    "        loss = criterion(predictions, batch.country)\n",
    "        \n",
    "        acc = categorical_accuracy(predictions, batch.country)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.description)\n",
    "            \n",
    "            loss = criterion(predictions, batch.country)\n",
    "            \n",
    "            acc = categorical_accuracy(predictions, batch.country)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.02 | Epoch Time: 1m 28s\n",
      "\tTrain Loss: 0.3627316837279698 | Train Acc: 87.80347768833317%\n",
      "\t Val. Loss: 0.4833411257485207 |  Val. Acc: 84.78194103194103%\n",
      "Epoch: 2.02 | Epoch Time: 1m 30s\n",
      "\tTrain Loss: 0.31593928699506313 | Train Acc: 89.2988230006432%\n",
      "\t Val. Loss: 0.507915745496164 |  Val. Acc: 84.59254709742872%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 2\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'wine-prediction-model.pt')\n",
    "    \n",
    "    print('Epoch: ' + str(epoch+1.02) + ' | Epoch Time: ' + str(epoch_mins) + 'm '+ str(epoch_secs) + 's')\n",
    "    print('\\tTrain Loss: ' + str(train_loss) + ' | Train Acc: ' + str(train_acc*100) + '%')\n",
    "    print('\\tVal. Loss: ' + str(valid_loss) + ' |  Val. Acc: ' + str(valid_acc*100) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.47960158129695984 | Test Acc: 85.12670765157606%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('wine-prediction-model.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print('Test Loss: ' + str(test_loss) + ' | Test Acc: '+ str(test_acc*100) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def predict_class(model, sentence, min_len = 4):\n",
    "    model.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    if len(tokenized) < min_len:\n",
    "        tokenized += ['<pad>'] * (min_len - len(tokenized))\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    preds = model(tensor)\n",
    "    max_preds = preds.argmax(dim = 1)\n",
    "    return max_preds.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class is: 2 = Italy\n"
     ]
    }
   ],
   "source": [
    "# Live testing: change the description to see how the model classifies it\n",
    "description = \"This opens with aromas suggesting resin, overripe plum, raisin, menthol and a whiff of nail polish remover. The palate showstart cranberry, star anise and a hint of dark baking spice alongside assertive, close-grained tannins leave an astringent finish. You'll also notice the slight warmth of alcohol on the finish.\"\n",
    "pred_class = predict_class(model, description)\n",
    "print('Predicted class is: ' + str(pred_class) + ' = ' + str(LABEL.vocab.itos[pred_class]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dark orange-pink in color, this meaty, substantial wine doesn't skimp on flavor or body. It presents a rewarding and intense celebration of raspberry and cherry that delights on the palate and will do well at the table, indoor or out.\n",
      "Actual: US, predicted: US\n",
      "\n",
      "This light and fruity wine offers fresh peach flavors, with a burst of lemon candy. It's off dry and finishes with a sugary kick.\n",
      "Actual: US, predicted: US\n",
      "\n",
      "Smooth, wood-polished wine, packed with a ripe, comfortable texture, very ripe red fruits, highlights of tannins. There are black figs, balanced with sweet acidity and fattened with some bacon flavors.\n",
      "Actual: Portugal, predicted: Portugal\n",
      "\n",
      "Produced by the team from classed growth Château Giscours, this is a successfully ripe wine with smoothly integrated tannins. Cushioned by the ripe fruit, the structure is concentrated and impressive. Drink this wine from 2022.\n",
      "Actual: France, predicted: France\n",
      "\n",
      "Bold and beautiful, this powerful Zin has a deep, dark color that's practically black but with a brilliant ruby rim. Brooding aromas suggest wood smoke, black pepper and boysenberry. There is an impressive density in texture, as firm tannins and acidity support the rich array of flavors.\n",
      "Actual: US, predicted: US\n",
      "\n",
      "Supple and silky, this expressive, seductive wine offers ripe plum and cherry fruit matched to polished tannins. It's finished with tasty barrel flavors of coconut, butter pecan and toast, and it is thoroughly, irresistibly delicious.\n",
      "Actual: US, predicted: US\n",
      "\n",
      "This Rhône-style blend is simple and fruity. The flavors of cherry jam, caramel and oak are balanced with acidity and smooth tannins. Sausages are a natural companion.\n",
      "Actual: US, predicted: US\n",
      "\n",
      "A massive wine, rich and forward in black cherry and raspberry jam, red currant, bacon and sandalwood flavors. Structural integrity fortunately provides balance by way of crisp acidity, firm tannins and a rocky minerality. Enjoyable now, after decanting, and should age through 2019.\n",
      "Actual: US, predicted: US\n",
      "\n",
      "Dried thyme and bay leaves impart a welcome herbal element to this wine's dark fruit. Hints of espresso, roasted meat and black olive help flesh out this slightly firm, linear blend of 80% Grenache, 15% Syrah and 5% Mourvèdre. With its crisp, dusty finish and admirable concentration, it look capable of aging through 2020.\n",
      "Actual: France, predicted: France\n",
      "\n",
      "This wine could easily be mistaken for French Chablis. It's notable for the tight, nervous feeling that acidity and minerals combine to stimulate, and the finish is very dry. There is, fortunately, a great heart of sweet tropical fruit, like the purest golden mango purée, ripe and succulent and honeyed. Terrific now, but such is the balance that this could be one of those Chardonnays that changes in interesting ways over the next 5–6 years.\n",
      "Actual: US, predicted: France\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check how the model performs on a batch\n",
    "import csv\n",
    "\n",
    "with open('preprocessed_datasets/test.csv') as f:\n",
    "    reader = csv.reader(f)\n",
    "    rows = []\n",
    "    for row in reader:\n",
    "        rows.append(row)\n",
    "\n",
    "for i in range(10):\n",
    "    row = rows[i]\n",
    "    sentence = row[2]\n",
    "    real_value = row[1]\n",
    "    pred_value = predict_class(model, sentence)\n",
    "    print(sentence)\n",
    "    print(\"Actual: \" + str(real_value) + \", predicted: \" + str(LABEL.vocab.itos[pred_value]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== ONLY NOTES BELOW =============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "print(vars(full_dataset.examples[0]))\n",
    "train_and_valid_data, test_data = full_dataset.split(random_state = random.seed(SEED))\n",
    "\n",
    "train_data, valid_data = train_and_valid_data.split(random_state = random.seed(SEED))\n",
    "print(len(train_data))\n",
    "print(len(test_data))\n",
    "print(len(valid_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wineproject",
   "language": "python",
   "name": "wineproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
