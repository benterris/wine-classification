{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchtext import data\n",
    "\n",
    "SEED = 1452 # for reproducibility\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "TEXT = data.Field(tokenize= 'spacy')\n",
    "LABEL = data.LabelField()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129976\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "with open('datasets/winemag-data-130k-v2.csv') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38992\n",
      "64988\n",
      "64987\n",
      "38992\n",
      "25996\n",
      "['18579,US,\"Flavors of candied lemon, lime and pineapple are brightened by crisp acidity in this unoaked  Sauvignon Blanc. The wine reflects this cool-climate Monterey appellation with its clean, brisk character.\",,85,16.0,California,Arroyo Seco,Central Coast,,,Mercy 2010 Sauvignon Blanc (Arroyo Seco),Sauvignon Blanc,Mercy\\n', '89421,US,\"There\\'s a lot of oak on this Chardonnay, to judge by the buttered toast and butterscotch richness. Underneath all that is a wine ripe in tropical fruits and green apples, brightened by excellent, mouthwatering acidity. The oak stands out now, but give the wine until 2015 or 2016 in the cellar to let the parts integrate.\",Sierra Mar Vineyard,90,40.0,California,Santa Lucia Highlands,Central Coast,,,Loring Wine Company 2012 Sierra Mar Vineyard Chardonnay (Santa Lucia Highlands),Chardonnay,Loring Wine Company\\n', '56113,US,\"Almost mauve in color, this widely distributed wine (named after the time the winemaking crew pops open their bottles at home) shows candied strawberry, watermelon Jolly Rancher and a touch of slate on the nose. Tons of tongue-tingling acidity and a expertly grippy texture cut through the watermelon and raspberry flavors.\",515 Vine Select,89,15.0,California,Central Coast,Central Coast,Matt Kettmann,@mattkettmann,Noble Vines 2016 515 Vine Select Rosé (Central Coast),Rosé,Noble Vines\\n']\n"
     ]
    }
   ],
   "source": [
    "# Split in train and test\n",
    "\n",
    "TEST_SET_SIZE = .3\n",
    "VALIDATION_SET_SIZE = .2\n",
    "\n",
    "indices = list(range(1, len(lines)))\n",
    "np.random.seed(SEED)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "first_split_index = int(TEST_SET_SIZE * len(lines))\n",
    "second_split_index = int((TEST_SET_SIZE+VALIDATION_SET_SIZE) * len(lines))\n",
    "\n",
    "print(first_split_index)\n",
    "print(second_split_index)\n",
    "\n",
    "test_indices = indices[:first_split_index]\n",
    "validation_indices = indices[first_split_index:second_split_index]\n",
    "train_indices = indices[second_split_index:]\n",
    "\n",
    "train_set = [lines[k] for k in train_indices]\n",
    "test_set = [lines[k] for k in test_indices]\n",
    "validation_set = [lines[k] for k in validation_indices]\n",
    "\n",
    "print(len(train_set))\n",
    "print(len(test_set))\n",
    "print(len(validation_set))\n",
    "print(train_set[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write split sets\n",
    "with open('preprocessed_datasets/train.csv', 'w') as train_file:\n",
    "    train_file.write(''.join(train_set))\n",
    "with open('preprocessed_datasets/test.csv', 'w') as test_file:\n",
    "    test_file.write(''.join(test_set))\n",
    "with open('preprocessed_datasets/validation.csv', 'w') as validation_file:\n",
    "    validation_file.write(''.join(validation_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the dataset\n",
    "\n",
    "# Put the thing we want to predict as a label\n",
    "tv_datafields = [(\"id\", None),\n",
    "                 (\"country\", LABEL),\n",
    "                 (\"description\", TEXT),\n",
    "                 (\"designation\", None),\n",
    "                 (\"points\", None),\n",
    "                 (\"price\", None),\n",
    "                 (\"province\", None),\n",
    "                 (\"region_1\", None),\n",
    "                 (\"region_2\", None),\n",
    "                 (\"taster_name\", None),\n",
    "                 (\"taster_twitter_handle\", None),\n",
    "                 (\"title\", None),\n",
    "                 (\"variety\", None),\n",
    "                 (\"winery\", None)]\n",
    "\n",
    "trn, vld, tst = data.TabularDataset.splits(path='preprocessed_datasets',\n",
    "                                     format=\"csv\",\n",
    "                                     train= 'train.csv',\n",
    "                                     validation='validation.csv',\n",
    "                                     test='test.csv',\n",
    "                                     fields=tv_datafields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25002\n",
      "47\n"
     ]
    }
   ],
   "source": [
    "# Prepare the vocab (BEWARE: this also downloads the vectors, ~800MB)\n",
    "MAX_VOCAB_SIZE = 25000\n",
    "TEXT.build_vocab(trn,\n",
    "                 max_size=MAX_VOCAB_SIZE,\n",
    "                 vectors = \"glove.6B.100d\", \n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "LABEL.build_vocab(trn)\n",
    "\n",
    "print(len(TEXT.vocab))\n",
    "# 25002 because of <pad> and <unk>\n",
    "print(len(LABEL.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function _default_unk_index at 0x7f85ef13fae8>, {'': 21, 'Chile': 5, 'China': 44, 'Serbia': 35, 'Austria': 7, 'South Africa': 10, 'Czech Republic': 32, 'Switzerland': 37, 'Canada': 14, 'US': 0, 'Greece': 13, 'Bosnia and Herzegovina': 38, 'Georgia': 22, 'Brazil': 27, 'Argentina': 6, 'Bulgaria': 16, 'Italy': 2, '90': 41, 'Bordeaux-style Red Blend': 43, 'Ukraine': 30, 'Peru': 31, 'Lebanon': 28, 'Mexico': 25, 'Macedonia': 34, 'Cyprus': 36, 'Romania': 17, 'Armenia': 42, 'Croatia': 24, 'Luxembourg': 39, 'India': 33, 'Germany': 9, 'Morocco': 29, 'England': 23, 'Australia': 8, 'Slovenia': 19, 'France': 1, 'Hungary': 15, 'Uruguay': 18, ' fine and extremely polished; hold for 10 years.\"': 40, 'Israel': 12, 'Egypt': 45, 'Turkey': 20, 'Spain': 3, 'Portugal': 4, 'Moldova': 26, 'New Zealand': 11, 'Slovakia': 46})\n"
     ]
    }
   ],
   "source": [
    "# print(LABEL.vocab.freqs.most_common(10))\n",
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the iterators\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (trn, vld, tst), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.description), # Sort the examples so the ones with similar lengths are close to each other\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
    "                 dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.convs = nn.ModuleList([\n",
    "                                    nn.Conv2d(in_channels = 1, \n",
    "                                              out_channels = n_filters, \n",
    "                                              kernel_size = (fs, embedding_dim)) \n",
    "                                    for fs in filter_sizes\n",
    "                                    ])\n",
    "        \n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        #text = [sent len, batch size]\n",
    "\n",
    "        text = text.permute(1, 0)        \n",
    "        #text = [batch size, sent len]\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        \n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        #embedded = [batch size, 1, sent len, emb dim]\n",
    "        \n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "        #conv_n = [batch size, n_filters, sent len - filter_sizes[n]]\n",
    "        \n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        #pooled_n = [batch size, n_filters]\n",
    "        \n",
    "        cat = self.dropout(torch.cat(pooled, dim = 1))\n",
    "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
    "            \n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [2,3,4]\n",
    "OUTPUT_DIM = len(LABEL.vocab)\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2604647\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5130,  1.2094,  0.6369,  ..., -0.4520, -0.4385,  0.0610],\n",
       "        [ 0.2348,  0.8024,  2.4040,  ..., -0.0284,  0.4618, -1.6748],\n",
       "        [-0.1077,  0.1105,  0.5981,  ..., -0.8316,  0.4529,  0.0826],\n",
       "        ...,\n",
       "        [-0.3868,  0.9669, -0.1847,  ...,  0.0047, -0.1571,  0.4996],\n",
       "        [ 0.5564, -2.1407,  1.5627,  ...,  0.2384, -0.6747, -0.3413],\n",
       "        [ 0.6508,  2.4764, -0.3841,  ..., -1.1424, -1.8912,  0.5773]])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
    "    correct = max_preds.squeeze(1).eq(y)\n",
    "    return correct.sum() / torch.FloatTensor([y.shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "#     print('start training')\n",
    "    \n",
    "    for batch in iterator:\n",
    "#         print(epoch_loss)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(batch.description)\n",
    "        \n",
    "        loss = criterion(predictions, batch.country)\n",
    "        \n",
    "        acc = categorical_accuracy(predictions, batch.country)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.description)\n",
    "            \n",
    "            loss = criterion(predictions, batch.country)\n",
    "            \n",
    "            acc = categorical_accuracy(predictions, batch.country)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.02 | Epoch Time: 1m 28s\n",
      "\tTrain Loss: 0.3627316837279698 | Train Acc: 87.80347768833317%\n",
      "\t Val. Loss: 0.4833411257485207 |  Val. Acc: 84.78194103194103%\n",
      "Epoch: 2.02 | Epoch Time: 1m 30s\n",
      "\tTrain Loss: 0.31593928699506313 | Train Acc: 89.2988230006432%\n",
      "\t Val. Loss: 0.507915745496164 |  Val. Acc: 84.59254709742872%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 2\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'wine-prediction-model.pt')\n",
    "    \n",
    "    print('Epoch: ' + str(epoch+1.02) + ' | Epoch Time: ' + str(epoch_mins) + 'm '+ str(epoch_secs) + 's')\n",
    "    print('\\tTrain Loss: ' + str(train_loss) + ' | Train Acc: ' + str(train_acc*100) + '%')\n",
    "    print('\\tVal. Loss: ' + str(valid_loss) + ' |  Val. Acc: ' + str(valid_acc*100) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.47960158129695984 | Test Acc: 85.12670765157606%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('wine-prediction-model.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print('Test Loss: ' + str(test_loss) + ' | Test Acc: '+ str(test_acc*100) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== ONLY NOTES BELOW =============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "print(vars(full_dataset.examples[0]))\n",
    "train_and_valid_data, test_data = full_dataset.split(random_state = random.seed(SEED))\n",
    "\n",
    "train_data, valid_data = train_and_valid_data.split(random_state = random.seed(SEED))\n",
    "print(len(train_data))\n",
    "print(len(test_data))\n",
    "print(len(valid_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wineproject",
   "language": "python",
   "name": "wineproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
