{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchtext import data\n",
    "\n",
    "SEED = 1452 # for reproducibility\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "TEXT = data.Field(tokenize= 'spacy')\n",
    "LABEL = data.LabelField()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 63 rows\n",
      "129909\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "import csv\n",
    "\n",
    "with open('datasets/winemag-data-130k-v2.csv') as f:\n",
    "    reader = csv.reader(f)\n",
    "    lines = []\n",
    "    \n",
    "    removed = 0\n",
    "\n",
    "    for row in reader:\n",
    "        variety = row[-2]\n",
    "        province = row[6]\n",
    "#         if not variety:\n",
    "#             print(row)\n",
    "#             removed +=1\n",
    "        if not province:\n",
    "            removed +=1\n",
    "        else:\n",
    "            lines.append(row)\n",
    "            \n",
    "            \n",
    "print(\"Removed \" + str(removed) + \" rows\")\n",
    "    \n",
    "    \n",
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38972\n",
      "64954\n",
      "64954\n",
      "38972\n",
      "25982\n",
      "[['10194', 'US', 'An exuberantly fruity, low-tannin Pinot with quite a bit of charm. Bursting with young, jammy wild-berry fruit, it tastes fruity and very spicy. Has those soft, juicy tannins that make it immediately drinkable, especially with spicy, salty foods.', \"Vintner's Reserve\", '87', '17.0', 'California', 'California', 'California Other', '', '', \"Kendall-Jackson 1998 Vintner's Reserve Pinot Noir (California)\", 'Pinot Noir', 'Kendall-Jackson'], ['99256', 'Argentina', \"Reedy and green on the nose, then choppy and clipped on the palate. For a full-priced Malbec, this is lacking in ripeness, body and finesse; it's prickly and limited, with green bean, rhubarb and cranberry flavors. Not bad but underwhelming for $33.\", 'Reserva', '84', '33.0', 'Mendoza Province', 'Mendoza', '', 'Michael Schachner', '@wineschach', 'Ksana 2008 Reserva Malbec (Mendoza)', 'Malbec', 'Ksana'], ['55433', 'US', \"There's no mistaking the variety, with the hallmark aromas of jalapeño pepper, cayenne and herbs. The ample cherry flavors are sweet, with the tannins bringing some graininess.\", 'Phinny Hill Vineyard', '89', '40.0', 'Washington', 'Horse Heaven Hills', 'Columbia Valley', 'Sean P. Sullivan', '@wawinereport', 'Tertulia 2012 Phinny Hill Vineyard Carmenère (Horse Heaven Hills)', 'Carmenère', 'Tertulia']]\n"
     ]
    }
   ],
   "source": [
    "# Split in train and test\n",
    "\n",
    "TEST_SET_SIZE = .3\n",
    "VALIDATION_SET_SIZE = .2\n",
    "\n",
    "indices = list(range(1, len(lines)))\n",
    "np.random.seed(SEED)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "first_split_index = int(TEST_SET_SIZE * len(lines))\n",
    "second_split_index = int((TEST_SET_SIZE+VALIDATION_SET_SIZE) * len(lines))\n",
    "\n",
    "print(first_split_index)\n",
    "print(second_split_index)\n",
    "\n",
    "test_indices = indices[:first_split_index]\n",
    "validation_indices = indices[first_split_index:second_split_index]\n",
    "train_indices = indices[second_split_index:]\n",
    "\n",
    "train_set = [lines[k] for k in train_indices]\n",
    "test_set = [lines[k] for k in test_indices]\n",
    "validation_set = [lines[k] for k in validation_indices]\n",
    "\n",
    "print(len(train_set))\n",
    "print(len(test_set))\n",
    "print(len(validation_set))\n",
    "print(train_set[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write split sets\n",
    "\n",
    "with open('preprocessed_datasets/train.csv', 'w') as train_file:\n",
    "#     train_file.write('\\n'.join(train_set))\n",
    "    writer = csv.writer(train_file)\n",
    "    writer.writerows(train_set)\n",
    "    \n",
    "with open('preprocessed_datasets/test.csv', 'w') as test_file:\n",
    "#     test_file.write('\\n'.join(test_set))\n",
    "    writer = csv.writer(test_file)\n",
    "    writer.writerows(test_set)\n",
    "with open('preprocessed_datasets/validation.csv', 'w') as validation_file:\n",
    "#     validation_file.write('\\n'.join(validation_set))\n",
    "    writer = csv.writer(validation_file)\n",
    "    writer.writerows(validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the dataset\n",
    "\n",
    "# Put the thing we want to predict as a label\n",
    "tv_datafields = [(\"id\", None),\n",
    "                 (\"country\", None),\n",
    "                 (\"description\", TEXT),\n",
    "                 (\"designation\", None),\n",
    "                 (\"points\", None),\n",
    "                 (\"price\", None),\n",
    "                 (\"province\", LABEL),\n",
    "                 (\"region_1\", None),\n",
    "                 (\"region_2\", None),\n",
    "                 (\"taster_name\", None),\n",
    "                 (\"taster_twitter_handle\", None),\n",
    "                 (\"title\", None),\n",
    "                 (\"variety\", None),\n",
    "                 (\"winery\", None)]\n",
    "\n",
    "trn, vld, tst = data.TabularDataset.splits(path='preprocessed_datasets',\n",
    "                                     format=\"csv\",\n",
    "                                     train= 'train.csv',\n",
    "                                     validation='validation.csv',\n",
    "                                     test='test.csv',\n",
    "                                     fields=tv_datafields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25002\n",
      "386\n"
     ]
    }
   ],
   "source": [
    "# Prepare the vocab (BEWARE: this also downloads the vectors, ~800MB)\n",
    "MAX_VOCAB_SIZE = 25000\n",
    "TEXT.build_vocab(trn,\n",
    "                 max_size=MAX_VOCAB_SIZE,\n",
    "                 vectors = \"glove.6B.100d\", \n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "LABEL.build_vocab(trn)\n",
    "\n",
    "print(len(TEXT.vocab))\n",
    "# 25002 because of <pad> and <unk>\n",
    "print(len(LABEL.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('California', 18224), ('Washington', 4255), ('Bordeaux', 3038), ('Tuscany', 3006), ('Oregon', 2622), ('Burgundy', 1935), ('Northern Spain', 1885), ('Piedmont', 1859), ('Mendoza Province', 1635), ('New York', 1391)]\n"
     ]
    }
   ],
   "source": [
    "print(LABEL.vocab.freqs.most_common(10))\n",
    "# print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the iterators\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (trn, vld, tst), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.description), # Sort the examples so the ones with similar lengths are close to each other\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
    "                 dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.convs = nn.ModuleList([\n",
    "                                    nn.Conv2d(in_channels = 1, \n",
    "                                              out_channels = n_filters, \n",
    "                                              kernel_size = (fs, embedding_dim)) \n",
    "                                    for fs in filter_sizes\n",
    "                                    ])\n",
    "        \n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        #text = [sent len, batch size]\n",
    "\n",
    "        text = text.permute(1, 0)        \n",
    "        #text = [batch size, sent len]\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        \n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        #embedded = [batch size, 1, sent len, emb dim]\n",
    "        \n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "        #conv_n = [batch size, n_filters, sent len - filter_sizes[n]]\n",
    "        \n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        #pooled_n = [batch size, n_filters]\n",
    "        \n",
    "        cat = self.dropout(torch.cat(pooled, dim = 1))\n",
    "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
    "            \n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [2,3,4]\n",
    "OUTPUT_DIM = len(LABEL.vocab)\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2706686\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5130,  1.2094,  0.6369,  ..., -0.4520, -0.4385,  0.0610],\n",
       "        [ 0.2348,  0.8024,  2.4040,  ..., -0.0284,  0.4618, -1.6748],\n",
       "        [-0.1077,  0.1105,  0.5981,  ..., -0.8316,  0.4529,  0.0826],\n",
       "        ...,\n",
       "        [-0.2476, -0.5630, -0.1649,  ..., -0.6803, -0.0308, -0.4074],\n",
       "        [-1.0221, -1.2177, -0.1291,  ..., -0.0973, -0.2656,  0.1390],\n",
       "        [ 0.7259, -0.7247, -1.2053,  ..., -0.9977, -0.8876,  0.7001]])"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
    "    correct = max_preds.squeeze(1).eq(y)\n",
    "    return correct.sum() / torch.FloatTensor([y.shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "#     print('start training')\n",
    "    \n",
    "    for batch in iterator:\n",
    "#         print(epoch_loss)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(batch.description)\n",
    "        \n",
    "        loss = criterion(predictions, batch.province)\n",
    "        \n",
    "        acc = categorical_accuracy(predictions, batch.province)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.description)\n",
    "            \n",
    "            loss = criterion(predictions, batch.province)\n",
    "            \n",
    "            acc = categorical_accuracy(predictions, batch.province)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.02 | Epoch Time: 1m 15s\n",
      "\tTrain Loss: 2.6688581638148268 | Train Acc: 41.71734329808522%\n",
      "\tVal. Loss: 1.9599077512184386 |  Val. Acc: 53.78806312095943%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'wine-prediction-model.pt')\n",
    "    \n",
    "    print('Epoch: ' + str(epoch+1.02) + ' | Epoch Time: ' + str(epoch_mins) + 'm '+ str(epoch_secs) + 's')\n",
    "    print('\\tTrain Loss: ' + str(train_loss) + ' | Train Acc: ' + str(train_acc*100) + '%')\n",
    "    print('\\tVal. Loss: ' + str(valid_loss) + ' |  Val. Acc: ' + str(valid_acc*100) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.47960158129695984 | Test Acc: 85.12670765157606%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('wine-prediction-model.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print('Test Loss: ' + str(test_loss) + ' | Test Acc: '+ str(test_acc*100) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def predict_class(model, sentence, min_len = 4):\n",
    "    model.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    if len(tokenized) < min_len:\n",
    "        tokenized += ['<pad>'] * (min_len - len(tokenized))\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    preds = model(tensor)\n",
    "    max_preds = preds.argmax(dim = 1)\n",
    "    return max_preds.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class is: 2 = Italy\n"
     ]
    }
   ],
   "source": [
    "# Live testing: change the description to see how the model classifies it\n",
    "description = \"This opens with aromas suggesting resin, overripe plum, raisin, menthol and a whiff of nail polish remover. The palate showstart cranberry, star anise and a hint of dark baking spice alongside assertive, close-grained tannins leave an astringent finish. You'll also notice the slight warmth of alcohol on the finish.\"\n",
    "pred_class = predict_class(model, description)\n",
    "print('Predicted class is: ' + str(pred_class) + ' = ' + str(LABEL.vocab.itos[pred_class]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dark orange-pink in color, this meaty, substantial wine doesn't skimp on flavor or body. It presents a rewarding and intense celebration of raspberry and cherry that delights on the palate and will do well at the table, indoor or out.\n",
      "Actual: US, predicted: US\n",
      "\n",
      "This light and fruity wine offers fresh peach flavors, with a burst of lemon candy. It's off dry and finishes with a sugary kick.\n",
      "Actual: US, predicted: US\n",
      "\n",
      "Smooth, wood-polished wine, packed with a ripe, comfortable texture, very ripe red fruits, highlights of tannins. There are black figs, balanced with sweet acidity and fattened with some bacon flavors.\n",
      "Actual: Portugal, predicted: Portugal\n",
      "\n",
      "Produced by the team from classed growth Château Giscours, this is a successfully ripe wine with smoothly integrated tannins. Cushioned by the ripe fruit, the structure is concentrated and impressive. Drink this wine from 2022.\n",
      "Actual: France, predicted: France\n",
      "\n",
      "Bold and beautiful, this powerful Zin has a deep, dark color that's practically black but with a brilliant ruby rim. Brooding aromas suggest wood smoke, black pepper and boysenberry. There is an impressive density in texture, as firm tannins and acidity support the rich array of flavors.\n",
      "Actual: US, predicted: US\n",
      "\n",
      "Supple and silky, this expressive, seductive wine offers ripe plum and cherry fruit matched to polished tannins. It's finished with tasty barrel flavors of coconut, butter pecan and toast, and it is thoroughly, irresistibly delicious.\n",
      "Actual: US, predicted: US\n",
      "\n",
      "This Rhône-style blend is simple and fruity. The flavors of cherry jam, caramel and oak are balanced with acidity and smooth tannins. Sausages are a natural companion.\n",
      "Actual: US, predicted: US\n",
      "\n",
      "A massive wine, rich and forward in black cherry and raspberry jam, red currant, bacon and sandalwood flavors. Structural integrity fortunately provides balance by way of crisp acidity, firm tannins and a rocky minerality. Enjoyable now, after decanting, and should age through 2019.\n",
      "Actual: US, predicted: US\n",
      "\n",
      "Dried thyme and bay leaves impart a welcome herbal element to this wine's dark fruit. Hints of espresso, roasted meat and black olive help flesh out this slightly firm, linear blend of 80% Grenache, 15% Syrah and 5% Mourvèdre. With its crisp, dusty finish and admirable concentration, it look capable of aging through 2020.\n",
      "Actual: France, predicted: France\n",
      "\n",
      "This wine could easily be mistaken for French Chablis. It's notable for the tight, nervous feeling that acidity and minerals combine to stimulate, and the finish is very dry. There is, fortunately, a great heart of sweet tropical fruit, like the purest golden mango purée, ripe and succulent and honeyed. Terrific now, but such is the balance that this could be one of those Chardonnays that changes in interesting ways over the next 5–6 years.\n",
      "Actual: US, predicted: France\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check how the model performs on a batch\n",
    "import csv\n",
    "\n",
    "with open('preprocessed_datasets/test.csv') as f:\n",
    "    reader = csv.reader(f)\n",
    "    rows = []\n",
    "    for row in reader:\n",
    "        rows.append(row)\n",
    "\n",
    "for i in range(10):\n",
    "    row = rows[i]\n",
    "    sentence = row[2]\n",
    "    real_value = row[1]\n",
    "    pred_value = predict_class(model, sentence)\n",
    "    print(sentence)\n",
    "    print(\"Actual: \" + str(real_value) + \", predicted: \" + str(LABEL.vocab.itos[pred_value]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== ONLY NOTES BELOW =============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "print(vars(full_dataset.examples[0]))\n",
    "train_and_valid_data, test_data = full_dataset.split(random_state = random.seed(SEED))\n",
    "\n",
    "train_data, valid_data = train_and_valid_data.split(random_state = random.seed(SEED))\n",
    "print(len(train_data))\n",
    "print(len(test_data))\n",
    "print(len(valid_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wineproject",
   "language": "python",
   "name": "wineproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
